42.33265081100001
171.20227522200003
41.703223535999996
170.12749302600002
38.36718500200004
169.69387190200007
33.83303657299996
170.908502162
41.28826178399993
170.84027415800006
40.41491774799988
169.4977517729999
[1] Train loss: 0.487, Train acc: 0.877
[1] Validation loss: 0.244, Validation acc: 0.933
36.59827510900004
170.0526519749999
38.338536763999855
170.40543817899993
39.38337399099987
169.71795133799992
40.02159678299995
169.57069810300027
39.236585190999904
170.05074390499976
[2] Train loss: 0.203, Train acc: 0.943
[2] Validation loss: 0.242, Validation acc: 0.922
36.746448011999746
170.8912918240003
41.05468906900023
170.05765087500004
41.0415329829998
170.21468734000018
40.649705494000045
170.35146752399987
44.75121128799992
171.01354849900008
[3] Train loss: 0.145, Train acc: 0.961
[3] Validation loss: 0.214, Validation acc: 0.931
41.37993835899988
170.41137156600007
37.682478644000184
171.2985438740002
41.65438693199985
170.19602278699995
38.80569095299961
170.40002354599983
43.938576819000446
171.74734576399987
[4] Train loss: 0.100, Train acc: 0.974
[4] Validation loss: 0.160, Validation acc: 0.951
39.11277503800011
170.82943364699986
35.8028305859998
170.17258676000074
37.74664532599945
171.69036180700004
38.768628600999364
170.65448030599964
40.466976076999345
171.03217873100039
41.554943610999544
170.77748237000014
[5] Train loss: 0.074, Train acc: 0.982
[5] Validation loss: 0.188, Validation acc: 0.944
36.6590252269998
170.78587207199962
39.128740948000086
171.6592407599992
36.934495669999706
170.16744749099962
39.629440187999535
171.05243779699958
38.38264631699985
171.43145690599977
[6] Train loss: 0.057, Train acc: 0.987
[6] Validation loss: 0.136, Validation acc: 0.960
36.861287800000355
170.32000594300007
32.67629405900061
170.6734717649997
39.413721605999854
171.33643045900044
40.73915319600019
171.24488393999945
40.15736930999992
171.0171199840006
[7] Train loss: 0.049, Train acc: 0.988
[7] Validation loss: 0.129, Validation acc: 0.960
36.93115120699986
170.6581623739994
37.1116772280011
170.7556434470007
36.41801737800051
170.73161307999908
37.719730594000794
171.1349276330002
36.194864355999016
170.30675244799932
[8] Train loss: 0.036, Train acc: 0.993
[8] Validation loss: 0.140, Validation acc: 0.959
33.5838613800006
171.44992832200114
35.979918934001034
171.23365276799996
35.82531496400043
171.49584189599955
35.846555257001455
171.50618504900012
35.176472402999934
170.9769242989987
35.29405686199971
170.78478453500065
[9] Train loss: 0.028, Train acc: 0.995
[9] Validation loss: 0.129, Validation acc: 0.961
36.40799854499892
171.6224874439995
35.28548955500082
171.0625629569986
33.74493642199923
171.9558136630003
33.2554429869997
172.0267447999995
30.813787690000026
171.32544898800006
[10] Train loss: 0.026, Train acc: 0.995
[10] Validation loss: 0.363, Validation acc: 0.919
37.88487101800092
172.2600127379992
35.41463464399931
171.82272196600024
32.66854385199986
171.505328792
36.43473749199984
171.57805938400088
37.0762189649995
172.04447412300033
[11] Train loss: 0.031, Train acc: 0.994
[11] Validation loss: 0.144, Validation acc: 0.956
31.854068179000024
171.57269842499954
35.21197594499972
170.95983230099955
33.32478993100085
170.82070550199933
38.81586675100152
171.38134976999936
27.607562313000017
171.9770043970002
[12] Train loss: 0.021, Train acc: 0.996
[12] Validation loss: 0.144, Validation acc: 0.956
31.905534186000295
171.4106373070008
33.52906607300065
171.62058220400104
32.03557579199878
171.1227742230003
33.488463625000804
171.37458428699938
28.399741589999394
171.48473797800034
32.69919982800093
170.9950122350001
[13] Train loss: 0.016, Train acc: 0.998
[13] Validation loss: 0.125, Validation acc: 0.963
34.647812350000095
172.3623436779999
34.08276477900108
172.1273140250014
30.703778682998745
172.41867230199932
31.361934749000284
171.68926616700082
32.00617465600044
172.03510007000114
[14] Train loss: 0.015, Train acc: 0.998
[14] Validation loss: 0.253, Validation acc: 0.931
30.831279999998515
171.706700662
35.15492873799849
171.17557981299979
33.78394445599952
171.56340902100055
34.38651336899966
173.41290699900128
30.887760816000082
171.7557497639973
[15] Train loss: 0.013, Train acc: 0.999
[15] Validation loss: 0.178, Validation acc: 0.949
33.56997546799903
171.7113280859994
33.87918286599961
172.28886734900152
32.431581254000776
173.08977480199974
31.13062129699756
173.30977666299805
34.46314134500062
172.272625177
[16] Train loss: 0.014, Train acc: 0.998
[16] Validation loss: 0.154, Validation acc: 0.956
31.461380030999862
172.08098709800106
32.439965523000865
171.67003842600025
32.701166217997525
171.85125368300214
34.89173334999941
171.02736597600233
34.10268344799988
172.3505933790002
[17] Train loss: 0.011, Train acc: 0.999
[17] Validation loss: 0.120, Validation acc: 0.965
31.077034679001372
171.71986312800072
32.55330289799895
172.25065316700056
28.266121190001286
171.8929745940004
32.26295174099869
171.56054718999803
32.93252640599894
172.81661557999905
28.89238209999894
171.74823676099913
[18] Train loss: 0.008, Train acc: 0.999
[18] Validation loss: 0.125, Validation acc: 0.962
29.07091653399766
172.33111889700012
33.693951534001826
172.55226670999764
30.489919155999814
172.92261188200064
34.69825667200348
173.07942518799973
27.60570754299988
172.050933489998
[19] Train loss: 0.009, Train acc: 0.999
[19] Validation loss: 0.363, Validation acc: 0.897
30.2712481890012
172.3783866679987
31.77895557500233
172.13476869299848
35.527251494000666
172.055259413999
33.19098768500044
172.48266819199853
33.19393265200051
172.71005586400133
[20] Train loss: 0.015, Train acc: 0.998
[20] Validation loss: 0.124, Validation acc: 0.964
32.73382930400112
172.77201219199924
30.3921049160017
172.9557291050005
34.69416823999927
172.4031448349997
32.278586792999704
173.24237542300034
30.45788430000175
173.1175755430013
[21] Train loss: 0.009, Train acc: 0.999
[21] Validation loss: 0.126, Validation acc: 0.965
34.38916696600063
173.02745626899923
36.1339906500034
172.15138584600209
32.07644216799963
172.24332264699842
33.69135135199758
172.16757343200152
31.171191231002013
171.95923583900003
34.10195260100227
173.11229728699982
[22] Train loss: 0.009, Train acc: 0.999
[22] Validation loss: 0.738, Validation acc: 0.837
At epoch 21, lr changed to 0.010000000000000002
31.061034375001327
173.56459322599767
30.2079647999999
172.98245240300093
33.82452999799716
172.8401477719999
31.525890240998706
172.79783200699967
32.232671798999945
173.432308337
[23] Train loss: 0.024, Train acc: 0.994
[23] Validation loss: 0.136, Validation acc: 0.962
31.53591177799899
173.49427408499832
32.88410033600303
173.2730866790007
32.869191583999054
172.39236452200203
31.23098885500076
173.22011196699896
33.73011285100074
172.54896443300095
[24] Train loss: 0.009, Train acc: 0.999
[24] Validation loss: 0.131, Validation acc: 0.963
32.7998678920012
173.2459515940027
33.956880473997444
172.4639176260025
35.26906494300056
174.00276382599986
32.506018836000294
173.52471898400108
32.44539589199849
172.55159149300016
[25] Train loss: 0.008, Train acc: 0.999
[25] Validation loss: 0.126, Validation acc: 0.965
33.40808820199891
175.6450618350027
32.37219478000043
172.2897417589993
32.91441047599801
175.4595861380003
32.985322189000726
174.14634120199844
31.223269141002675
173.66065837099814
32.687897529998736
173.36487638099788
[26] Train loss: 0.006, Train acc: 1.000
[26] Validation loss: 0.124, Validation acc: 0.966
33.0225258069986
176.30939787399984
33.33355582800141
176.53229807800017
33.34768291999717
176.43907570800002
34.68480271700173
176.47962720000214
33.878788154001086
206.41239002099974
[27] Train loss: 0.007, Train acc: 0.999
[27] Validation loss: 0.124, Validation acc: 0.965
48.21829565900043
191.14471174900245
55.77072107999993
186.8880995349973
59.18817655000021
189.00668629999927
70.39750029700008
185.8355227360007
73.24997982600325
190.65905862
[28] Train loss: 0.006, Train acc: 1.000
[28] Validation loss: 0.121, Validation acc: 0.967
70.61386549700183
202.0848438110006
76.42792847000237
211.1997279290008
79.05549864299974
346.70313987300324
48.913691306002875
337.00447911800075
53.14232333600012
308.4727381130033
[29] Train loss: 0.007, Train acc: 1.000
[29] Validation loss: 0.118, Validation acc: 0.967
42.592243184000836
244.3424330659982
34.995248854997044
173.6596156939995
34.73535968599754
175.4890847429997
34.662581251999654
176.7862715789961
32.546964895998826
175.59232792099647
[30] Train loss: 0.006, Train acc: 0.999
[30] Validation loss: 0.121, Validation acc: 0.965
33.49418395599787
176.90525900300418
33.34179262800171
176.35107283300022
34.61991385299916
186.9255870229972
33.94323974100553
183.95428438499948
33.81916656700196
174.0269940990038
31.766010774997994
177.2388665230028
[31] Train loss: 0.005, Train acc: 1.000
[31] Validation loss: 0.124, Validation acc: 0.966
34.08888109799591
188.26246647399967
34.284668066997256
188.31551051099814
35.41310315800365
188.67183755100268
34.18255141499685
188.5262495049974
36.49243060500157
184.01584162500512
[32] Train loss: 0.004, Train acc: 1.000
[32] Validation loss: 0.118, Validation acc: 0.967
34.34481720199983
177.18895021900244
35.31767368400324
177.39911378800025
34.55763044700143
179.19541495500016
33.824621036997996
178.71852332500566
34.725019328005146
175.50670400799572
[33] Train loss: 0.004, Train acc: 1.000
[33] Validation loss: 0.122, Validation acc: 0.965
30.393073307001032
178.10784900499857
33.022163585003
179.6517885650028
31.40345324399823
180.88438675700309
32.218303986002866
176.87996940599987
33.188056494997
178.71300028199767
[34] Train loss: 0.004, Train acc: 1.000
[34] Validation loss: 0.120, Validation acc: 0.967
At epoch 33, lr changed to 0.0010000000000000002
34.631355869998515
174.32059230400046
32.5112936459991
174.37795577199722
32.245412442003726
173.1382199419968
34.02267770999606
173.3401879869998
34.11173493899696
173.84790245500335
33.642360225996526
174.04291230599483
[35] Train loss: 0.004, Train acc: 1.000
[35] Validation loss: 0.121, Validation acc: 0.968
32.55759277900506
173.53590368299774
34.80615020899859
173.65716748100385
32.2563755930023
173.34184361299413
32.299638336997305
173.91727763100062
34.0937863729996
173.67554466
[36] Train loss: 0.004, Train acc: 1.000
[36] Validation loss: 0.119, Validation acc: 0.967
32.61981551000645
175.4135961869979
35.01324387600471
175.96841725300328
34.92138729200087
178.3197587320028
35.59920948099898
174.07653333800408
36.20251906700287
173.95830955999554
[37] Train loss: 0.004, Train acc: 1.000
[37] Validation loss: 0.120, Validation acc: 0.967
33.61227311100083
176.3173061280031
32.95255021099729
174.24055789800332
32.42194680499961
186.79346943500423
34.08379021099972
186.5894609770039
36.00602375999733
186.9947532389997
[38] Train loss: 0.004, Train acc: 1.000
[38] Validation loss: 0.118, Validation acc: 0.967
35.4433504089975
186.11394464300247
35.97532476000197
187.06097506000515
34.86866457200085
187.120677340994
35.69127791200299
186.97772861500562
35.32701256600558
176.06922736100387
33.65159388000029
174.13701443299942
[39] Train loss: 0.005, Train acc: 1.000
[39] Validation loss: 0.122, Validation acc: 0.967
34.38722448799672
174.94633488300315
33.58370289400045
173.64736316900235
31.870598224995774
173.83434310000303
34.38886575799552
173.37104263599758
33.30072142800054
174.14217344200006
[40] Train loss: 0.004, Train acc: 1.000
[40] Validation loss: 0.118, Validation acc: 0.967
Best_val_acc for lr = 0.1, sigma_w = 1.0, sigma_b = 0is 0.9677
